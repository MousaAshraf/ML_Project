{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139650b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bc3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Created folder: {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4ab342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder, count):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "\n",
    "        if image.size[0] < 500 or image.size[1] < 500:\n",
    "            print(f\"Skipped small image: {image.size}\")\n",
    "            return False\n",
    "\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = image.resize((800, 800))\n",
    "        path = os.path.join(folder, f\"image_{count}.jpg\")\n",
    "        image.save(path)\n",
    "        print(f\"Saved: {path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a813fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images_bing(query, num_images, folder_name):\n",
    "    print(f\"Searching for: {query}\")\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    url = f\"https://www.bing.com/images/search?q={query}&form=HDRSC2\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    img_tags = soup.find_all(\"a\", class_=\"iusc\")\n",
    "    image_urls = []\n",
    "\n",
    "    for tag in img_tags:\n",
    "        try:\n",
    "            m_json = tag.get(\"m\")\n",
    "            m_url = eval(m_json)[\"murl\"]\n",
    "            if m_url.startswith(\"http\"):\n",
    "                image_urls.append(m_url)\n",
    "        except:\n",
    "            continue\n",
    "        if len(image_urls) >= num_images:\n",
    "            break\n",
    "\n",
    "    print(f\"Found {len(image_urls)} images.\")\n",
    "\n",
    "    count = 0\n",
    "    for url in image_urls:\n",
    "        if download_image(url, folder_name, count):\n",
    "            count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8a4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(\"dataset/street_pothole\")\n",
    "create_folder(\"dataset/normal_street\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19986e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_images_bing(\"street pothole\", 300, \"dataset/street_pothole\")\n",
    "scrape_images_bing(\"clean city street road\", 300, \"dataset/normal_street\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
